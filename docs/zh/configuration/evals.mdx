---
title: 评估与度量
sidebarTitle: 评估
description: 监控性能、优化成本，并评估 LLM 的有效性
---

评估可帮助你了解自动化的表现、确定哪些模型最适合你的用例，以及如何在成本与可靠性之间实现优化。本指南同时涵盖对自身工作流的监控与开展全面评估的方法。

## 为什么评估至关重要 {#why-evaluations-matter}

- **性能优化**：找出最适合你特定自动化任务的模型与配置
- **成本控制**：跟踪 token 使用量与推理时长，优化支出
- **可靠性**：衡量成功率并定位失败模式
- **模型选择**：在真实任务上横向比较不同 LLM，做出更有依据的决策

<Card title="实时模型对比" icon="scale-balanced" href="https://www.stagehand.dev/evals">
  在 [Stagehand 评估套件与仪表板](https://www.stagehand.dev/evals) 上查看不同 LLM 的实时性能对比
</Card>

## 全面评估 {#comprehensive-evaluations}

评估可帮助你系统化地测试并改进自动化工作流。Stagehand 提供内置评估以及用于创建自定义评估的工具。

<Tip>
  要运行评估（evals），你需要克隆 [Stagehand 仓库](https://github.com/browserbase/stagehand) 并运行 `npm install` 安装依赖。
</Tip>

我们提供两种类型的评估：

1. 确定性评估（Deterministic Evals）——无需任何 LLM 推理即可运行，结果可复现。
2. 基于 LLM 的评估（LLM-based Evals）——用于测试 Stagehand 的 AI 基元的底层功能。

### 基于 LLM 的评估 {#llm-based-evals}

<Tip>
  要运行 LLM 评估，你需要一个 [Braintrust 账户](https://www.braintrust.dev/docs/)。
</Tip>

要运行基于 LLM 的评估，你可以在 Stagehand 仓库中执行 `npm run evals`。这会测试 Stagehand 中的 LLM 基元是否按预期工作。

评估分为以下类别：

1. Act 评估——测试 `act` 方法的功能。
2. Extract 评估——测试 `extract` 方法的功能。
3. Observe 评估——测试 `observe` 方法的功能。
4. 组合评估（Combination Evals）——联合测试 `act`、`extract` 和 `observe` 方法的功能。

#### 配置与运行评估 {#configuring-and-running-evals}

你可以在 [`evals/tasks`](https://github.com/browserbase/stagehand/tree/main/evals/tasks) 查看具体评估。每个评估会基于 [`evals/evals.config.json`](https://github.com/browserbase/stagehand/blob/main/evals/evals.config.json) 归类到相应类别。你可以在 [`evals/taskConfig.ts`](https://github.com/browserbase/stagehand/blob/main/evals/taskConfig.ts) 中指定要运行的模型及其他通用任务配置。

要运行特定评估，执行 `npm run evals <eval>`；要运行某一类别中的全部评估，执行 `npm run evals category <category>`。

#### 查看评估结果 {#viewing-eval-results}

![Eval results](/images/evals.png)

评估结果可在 Braintrust 上查看。运行 `npm run evals` 后，终端会输出一个 Braintrust 链接，你可通过该链接查看特定评估的结果。

默认情况下，每个评估会在每个模型上运行五次。“Exact Match” 列显示评估结果完全匹配的比例；“Error Rate” 列显示评估出错的比例。

你可以使用 Braintrust 的 UI 按模型/评估进行筛选，并在所有评估间聚合结果。

### 确定性评估 {#deterministic-evals}

要运行确定性评估，只需在 Stagehand 仓库中执行 `npm run e2e`。这将测试 Stagehand 中对 Playwright 的集成是否按预期工作。

这些测试位于 [`evals/deterministic`](https://github.com/browserbase/stagehand/tree/main/evals/deterministic)，会在 Browserbase 浏览器和本地无头 Chromium 浏览器上运行。

## 创建自定义评估 {#creating-custom-evaluations}

### 操作步骤 {#step-by-step-guide}

<Steps>
  <Step title="创建评估文件">
    在 `evals/tasks/your-eval.ts` 中新建文件：

    ```typescript
    import { EvalTask } from '../types';

    export const customEvalTask: EvalTask = {
      name: 'custom_task_name',
      description: 'Test specific automation workflow',
      
      // 测试初始化
      setup: async ({ page }) => {
        await page.goto('https://example.com');
      },
      
      // 实际测试
      task: async ({ stagehand, page }) => {
        // 你的自动化逻辑
        await stagehand.act({ action: 'click the login button' });
        const result = await stagehand.extract({ 
          instruction: 'Get the user name',
          schema: { username: 'string' }
        });
        return result;
      },
      
      // 校验
      validate: (result, expected) => {
        return result.username === expected.username;
      },
      
      // 测试用例
      testCases: [
        {
          input: { /* test input */ },
          expected: { username: 'john_doe' }
        }
      ],
      
      // 评估标准
      scoring: {
        exactMatch: true,
        timeout: 30000,
        retries: 2
      }
    };
    ```
  </Step>

  <Step title="添加到配置">
    更新 `evals/evals.config.json`：

    ```json
    {
      "categories": {
        "custom": ["custom_task_name"],
        "existing_category": ["custom_task_name"]
      }
    }
    ```
  </Step>

  <Step title="运行评估">
    ```bash
    # 测试你的自定义评估
    npm run evals custom_task_name

    # 运行整个 custom 类别
    npm run evals category custom
    ```
  </Step>
</Steps>

## 自定义评估的最佳实践 {#best-practices-for-custom-evals}

<AccordionGroup>
  <Accordion title="测试设计原则">
    - **原子性**：每个测试应只验证一项特定能力
    - **确定性**：测试应产生一致的结果
    - **贴近真实**：使用真实世界的场景与网站
    - **可度量**：定义清晰的成功/失败判定标准
  </Accordion>

  <Accordion title="性能优化">
    - **并行执行**：将测试设计为可独立运行
    - **资源管理**：每个测试结束后及时清理
    - **超时处理**：为各类操作设置合适的超时时间
    - **错误恢复**：以可控方式处理失败
  </Accordion>

  <Accordion title="数据质量">
    - **基准真值**：建立可靠的期望结果
    - **边界用例**：覆盖边界条件与错误场景
    - **统计显著性**：多次运行以提高可靠性
    - **版本控制**：持续跟踪测试用例的变更
  </Accordion>
</AccordionGroup>

### 评估故障排除 {#troubleshooting-evaluations}

<AccordionGroup>
  <Accordion title="评估超时">
    **症状**：测试因超时错误而失败

    **解决方案**：

    - 在 `taskConfig.ts` 中增加超时时间
    - 使用更快的模型（Gemini 2.5 Flash、GPT-4o Mini）
    - 优化测试场景以降低复杂度
    - 检查与 LLM 提供商的网络连通性
  </Accordion>

  <Accordion title="结果不一致">
    **症状**：同一测试随机通过或失败

    **解决方案**：

    - 将 temperature 设置为 0 以获得确定性输出
    - 提高重复次数以达到统计显著性
    - 对复杂任务使用更强的模型
    - 检查网站动态内容对测试的影响
  </Accordion>

  <Accordion title="评估成本高">
    **症状**：Token 使用量超出预算

    **解决方案**：

    - 使用更具性价比的模型（Gemini 2.0 Flash、GPT-4o Mini）
    - 在初始测试阶段减少重复次数
    - 聚焦特定评估类别
    - 使用本地浏览器环境以降低 Browserbase 成本
  </Accordion>

  <Accordion title="Braintrust 集成问题">
    **症状**：结果未上传到仪表板

    **解决方案**：

    - 检查 Braintrust API 密钥配置
    - 验证网络连接
    - 将 Braintrust SDK 更新至最新版本
    - 在 Braintrust 仪表板中检查项目权限
  </Accordion>
</AccordionGroup>