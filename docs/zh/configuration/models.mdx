---
title: 模型
sidebarTitle: 模型
description: 使用 LLM 增强 Stagehand，在性能、成本与可靠性间实现最佳效果
---

Stagehand 使用大型语言模型（LLM）来理解网页、规划操作，并与复杂界面交互。所选 LLM 将显著影响你的自动化在准确性、速度与成本方面的表现。

<Card title="模型评估" href="https://www.stagehand.dev/evals" icon="paper-plane">
  请访问我们的模型评估页面，了解如何选择合适模型的更多信息。
</Card>

## 为什么选择 LLM 至关重要 {#why-llm-choice-matters}

- **准确性**：更强的模型能提供更可靠的元素检测与动作规划
- **速度**：更快的模型可降低自动化延迟
- **成本**：不同提供商的定价结构各不相同
- **可靠性**：对结构化输出的良好支持可确保自动化行为一致

<Tip>
  关于如何选择合适的模型，详见我们的 [模型评估](https://www.stagehand.dev/evals) 页面。
</Tip>

<Warning>
  **Ollama** 上的小模型在稳定生成结构化输出方面表现不佳。尽管技术上受支持，但我们不建议在生产级 Stagehand 工作流中使用它们。
</Warning>

## 环境变量设置 {#environment-variables-setup}

在配置 Stagehand 之前，请先设置 API 密钥：

<CodeGroup>
  ```bash .env
  # 选择一个或多个模型提供方
  OPENAI_API_KEY=your_openai_key_here
  ANTHROPIC_API_KEY=your_anthropic_key_here
  GOOGLE_API_KEY=your_google_key_here
  GROQ_API_KEY=your_groq_key_here
  ```
</CodeGroup>

## 支持的提供商 {#supported-providers}

Stagehand 支持具备结构化输出能力的主流 LLM 提供商：

### 生产可用的提供商 {#production-ready-providers}

| 提供商 | 最佳模型 | 优势 | 使用场景 |
|----------|-------------|-----------|----------|
| **OpenAI** | `gpt-4.1`、`gpt-4.1-mini` | 高准确率、可靠性强 | 生产环境、复杂站点 |
| **Anthropic** | `claude-3-7-sonnet-latest` | 推理能力出色 | 复杂自动化任务 |
| **Google** | `gemini-2.5-flash`、`gemini-2.5-pro` | 速度快、成本友好 | 大规模自动化 |

### 其他提供商 {#additional-providers}

<Expandable title="更多提供商">
  - **Groq** - `llama-3.3-70b-versatile`（适合对速度要求严苛的应用）
  - **xAI** - `grok-beta`（适合复杂推理）
  - **Azure** - 企业级 OpenAI 部署
  - **Cerebras** - 高速推理
  - **TogetherAI** - 开源模型
  - **Mistral** - `mixtral-8x7b-32768`（欧洲选项）
  - **DeepSeek** - 高性价比替代方案
  - **Perplexity** - 实时网页数据
  - **Ollama** - 本地部署（准确性有限）
  - **运行 AI SDK 中包含的任意模型** - 在 [Vercel AI SDK](https://sdk.vercel.ai/providers/ai-sdk-providers) 中查看支持的模型（按照[此指南](#vercel-ai-sdk)开始使用。）
</Expandable>

## 基本配置 {#basic-configuration}

### 模型名称格式 {#model-name-format}

Stagehand 使用 `provider/model-name` 格式来指定模型。

**示例：**

- OpenAI：`openai/gpt-4.1`
- Anthropic：`anthropic/claude-3-7-sonnet-latest`
- Google：`google/gemini-2.5-flash`（推荐）

### 快速开始示例 {#quick-start-examples}

<Tabs>
  <Tab title="Google（推荐）">
    <CodeGroup>
      ```typescript TypeScript
      import { Stagehand } from "@browserbasehq/stagehand";

      const stagehand = new Stagehand({
        modelName: "google/gemini-2.5-flash",
        modelClientOptions: {
          apiKey: process.env.GOOGLE_API_KEY,
        },
      });
      ```

      ```python Python
      import os
      from stagehand import Stagehand

      stagehand = Stagehand(
          model_name="google/gemini-2.5-flash",
          model_api_key=os.getenv("GOOGLE_API_KEY")
      )
      ```
    </CodeGroup>
  </Tab>

  <Tab title="OpenAI">
    <CodeGroup>
      ```typescript TypeScript
      import { Stagehand } from "@browserbasehq/stagehand";

      const stagehand = new Stagehand({
        modelName: "openai/gpt-4.1",
        modelClientOptions: {
          apiKey: process.env.OPENAI_API_KEY,
        },
      });
      ```

      ```python Python
      import os
      from stagehand import Stagehand

      stagehand = Stagehand(
          model_name="openai/gpt-4.1",
          model_api_key=os.getenv("OPENAI_API_KEY")
      )
      ```
    </CodeGroup>
  </Tab>

  <Tab title="Anthropic">
    <CodeGroup>
      ```typescript TypeScript
      import { Stagehand } from "@browserbasehq/stagehand";

      const stagehand = new Stagehand({
        modelName: "anthropic/claude-3-7-sonnet-latest",
        modelClientOptions: {
          apiKey: process.env.ANTHROPIC_API_KEY,
        },
      });
      ```

      ```python Python
      import os
      from stagehand import Stagehand

      stagehand = Stagehand(
          model_name="anthropic/claude-3-7-sonnet-latest",
          model_api_key=os.getenv("ANTHROPIC_API_KEY")
      )
      ```
    </CodeGroup>
  </Tab>
</Tabs>

## 自定义 LLM 集成 {#custom-llm-integration}

<Note>
  自定义 LLM 目前仅支持 TypeScript。
</Note>

通过自定义客户端将任意 LLM 集成到 Stagehand。唯一要求是具备对**结构化输出**的支持，以确保自动化行为一致。

### Vercel AI SDK {#vercel-ai-sdk}

[Vercel AI SDK](https://sdk.vercel.ai/providers/ai-sdk-providers) 是一个常用的 LLM 交互库。你可以使用 Vercel AI SDK 支持的任意提供商为你的模型创建客户端，**前提是该提供商支持结构化输出**。

Vercel AI SDK 支持 OpenAI、Anthropic、Google 的提供商，并同时支持 **Amazon Bedrock** 和 **Azure OpenAI**。

开始之前，你需要安装 `ai` 包以及所选提供商的包。例如，要使用 Amazon Bedrock，需要安装 `@ai-sdk/amazon-bedrock` 包。

你还需要参考 [Vercel AI SDK external client](https://github.com/browserbase/stagehand/blob/main/examples/external_clients/aisdk.ts) 作为模板，为你的模型创建客户端。

<Tabs>
  <Tab title="npm">
    ```bash
    npm install ai @ai-sdk/amazon-bedrock
    ```
  </Tab>

  <Tab title="pnpm">
    ```bash
    pnpm install ai @ai-sdk/amazon-bedrock
    ```
  </Tab>

  <Tab title="yarn">
    ```bash
    yarn add ai @ai-sdk/amazon-bedrock
    ```
  </Tab>
</Tabs>

入门时，你可以使用 [Vercel AI SDK external client](https://github.com/browserbase/stagehand/blob/84f810b4631291307a32a47addad7e26e9c1deb3/examples/external_clients/aisdk.ts) 作为模板，为你的模型创建客户端。

```ts
// 安装并导入你要使用的提供商。
// 例如，若使用 OpenAI，请从 @ai-sdk/openai 导入 `openai`
import { bedrock } from "@ai-sdk/amazon-bedrock";
import { AISdkClient } from "./external_clients/aisdk";

const stagehand = new Stagehand({
  llmClient: new AISdkClient({
	model: bedrock("anthropic.claude-3-7-sonnet-20250219-v1:0"),
  }),
});
```

## 故障排查 {#troubleshooting}

### 常见问题 {#common-issues}

<AccordionGroup>
  <Accordion title="模型不支持结构化输出">
    **错误**: `Model does not support structured outputs`

    **解决方案**:
    使用支持函数调用/结构化输出的模型。最低要求如下：

    - 模型必须支持 JSON/结构化输出
    - 模型必须具备较强的推理能力
    - 模型必须能够处理复杂指令

    针对各个提供商，使用满足这些要求的最新模型。例如：

    - **OpenAI**：GPT-4 系列或更新
    - **Anthropic**：Claude 3 系列或更新
    - **Google**：Gemini 2 系列或更新
    - **其他提供商**：支持结构化输出的最新模型

    **注意**：避免使用不具备结构化输出能力或未针对指令跟随微调的基础语言模型。如有疑问，请查看我们的 [Model Evaluation](https://www.stagehand.dev/evals) 页面以获取最新推荐。
  </Accordion>

  <Accordion title="身份验证错误">
    **错误**: `Invalid API key` 或 `Unauthorized`

    **解决方案**:

    - 核对环境变量是否正确配置
    - 检查 API key 的权限与配额
    - 确保你使用的是该提供商对应的正确 API key
    - 对于 Anthropic，确保你已开通 Claude API 访问权限
  </Accordion>

  <Accordion title="自动化结果不一致">
    **症状**: 操作时好时坏

    **原因与解决方案**:

    - **模型能力不足**：改用更强的模型——查看我们的 [Model Evaluation](https://www.stagehand.dev/evals) 页面获取当前推荐
    - **temperature 过高**：将 temperature 设为 0 以获得确定性输出
    - **页面复杂**：切换到在我们 [Model Evaluation](https://www.stagehand.dev/evals) 页面上准确率更高的模型
    - **速率限制**：实现带指数退避的重试逻辑
    - **上下文限制**：降低页面复杂度或使用具备更大上下文窗口的模型
    - **提示不清晰**：确保自动化指令清晰且具体
  </Accordion>

  <Accordion title="性能缓慢">
    **问题**: 自动化响应耗时过长

    **解决方案**:

    - **使用快速模型**：选择为速度优化的模型
      - 响应时间 < 1s 的任意模型
      - 带有“fast”或“flash”变体的模型
    - **优化设置**:
      - 使用 `verbose: 0` 以最小化 token 使用
      - 将 temperature 设为 0 以获得最快处理
      - 将最大 tokens 设为尽可能低
    - **考虑本地部署**：本地模型可提供最低延迟
    - **批量操作**：在可能时合并多个动作
  </Accordion>

  <Accordion title="成本过高">
    **问题**: LLM 使用成本过高

    **成本优化策略**:

    1. **切换到高性价比模型**：
       - 查看我们的 [Model Evaluation](https://www.stagehand.dev/evals) 页面，获取最新的成本-性能基准
       - 选择每 token 成本更低且仍满足准确性要求的模型
       - 考虑选择为速度优化的模型以降低总体运行成本
    2. **优化 token 使用**：
       - 设置 `verbose: 0` 以减少日志开销
       - 使用精炼的提示并限制响应长度
    3. **智能模型选择**：先用更便宜的模型，仅在需要时回退到高端模型
    4. **缓存响应**：为重复的自动化模式实施 LLM 响应缓存
    5. **监控使用**：设置计费告警，并按每次自动化运行跟踪成本
    6. **批量处理**：将多个相似任务一并处理
  </Accordion>
</AccordionGroup>

### 下一步 {#next-steps}

<CardGroup cols={2}>
  <Card title="选择模型" href="https://www.stagehand.dev/evals" icon="robot">
    查看我们的 Model Evaluation 页面
  </Card>

  <Card title="测试模型" href="/zh/configuration/evals" icon="flask-vial">
    在我们的 Model Evaluation 指南中评估你特定用例的性能
  </Card>

  <Card title="跟踪成本" href="/zh/configuration/observability" icon="chart-line">
    使用我们的 Observability 工具监控 token 使用并设置警报
  </Card>

  <Card title="缓存结果" href="/zh/best-practices/caching" icon="database">
    参考我们的 Caching 指南存储成功的模式
  </Card>
</CardGroup>